{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYVZaaEyPdOW",
        "outputId": "fb53aaa6-a9f1-463f-d1d4-b418331979ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbEhwlIVUvmi",
        "outputId": "7d854a59-a5ac-4901-e12d-27320c774e06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import wandb\n",
        "import tiktoken\n",
        "\n",
        "# Set MPS memory management\n",
        "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
        "os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.5'\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"shakespeare-gpt\", name=\"gpt2-124M-training\")\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                           .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 512\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)\n",
        "        y = (buf[1:]).view(B, T)\n",
        "        self.current_position += B*T\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "# Device configuration\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Initialize model and move to device\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "# Initialize data loader\n",
        "train_loader = DataLoaderLite(B=4, T=32)\n",
        "\n",
        "# Training settings\n",
        "learning_rate = 3e-4\n",
        "num_iters = 100000  # Increased to 100000\n",
        "eval_interval = 50   # Evaluate every 50 iterations\n",
        "best_loss = float('inf')\n",
        "checkpoint_dir = 'checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"\\n=== Starting Training ===\")\n",
        "print(f\"Total iterations: {num_iters}\")\n",
        "print(f\"Evaluation interval: {eval_interval}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "\n",
        "# Training loop\n",
        "for iter in range(num_iters):\n",
        "    # Get batch\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log progress every 50 iterations\n",
        "    if iter % eval_interval == 0:\n",
        "        current_loss = loss.item()\n",
        "        print(f'step {iter}, loss: {current_loss:.4f}')\n",
        "        wandb.log({\n",
        "            \"iter\": iter,\n",
        "            \"loss\": current_loss\n",
        "        })\n",
        "\n",
        "        # Save if this is the best model so far\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'model_step_{iter}_loss_{current_loss:.4f}.pt')\n",
        "            torch.save({\n",
        "                'iter': iter,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': current_loss,\n",
        "                'best_loss': best_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f'New best model saved! Loss: {current_loss:.4f}')\n",
        "\n",
        "            # Also save as best model\n",
        "            torch.save({\n",
        "                'iter': iter,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': current_loss,\n",
        "                'best_loss': best_loss,\n",
        "            }, 'best_model.pt')\n",
        "\n",
        "print(\"\\n=== Training Complete ===\")\n",
        "print(f\"Best loss achieved: {best_loss:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "final_path = os.path.join(checkpoint_dir, 'model_final.pt')\n",
        "torch.save({\n",
        "    'iter': num_iters-1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "    'best_loss': best_loss,\n",
        "}, final_path)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL_4a32grZ4f",
        "outputId": "44f2e84b-545f-40a3-87df-8c1e92fbcc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 2640 batches\n",
            "\n",
            "=== Starting Training ===\n",
            "Total iterations: 100000\n",
            "Evaluation interval: 50\n",
            "Learning rate: 0.0003\n",
            "step 0, loss: 11.0543\n",
            "New best model saved! Loss: 11.0543\n",
            "step 50, loss: 7.2089\n",
            "New best model saved! Loss: 7.2089\n",
            "step 100, loss: 6.2097\n",
            "New best model saved! Loss: 6.2097\n",
            "step 150, loss: 6.1574\n",
            "New best model saved! Loss: 6.1574\n",
            "step 200, loss: 5.1580\n",
            "New best model saved! Loss: 5.1580\n",
            "step 250, loss: 5.3966\n",
            "step 300, loss: 6.0775\n",
            "step 350, loss: 5.0090\n",
            "New best model saved! Loss: 5.0090\n",
            "step 400, loss: 4.2791\n",
            "New best model saved! Loss: 4.2791\n",
            "step 450, loss: 5.3249\n",
            "step 500, loss: 6.1206\n",
            "step 550, loss: 4.5425\n",
            "step 600, loss: 5.9818\n",
            "step 650, loss: 5.7832\n",
            "step 700, loss: 4.8834\n",
            "step 750, loss: 6.1132\n",
            "step 800, loss: 4.5917\n",
            "step 850, loss: 5.6705\n",
            "step 900, loss: 6.1013\n",
            "step 950, loss: 5.5465\n",
            "step 1000, loss: 5.0481\n",
            "step 1050, loss: 5.6484\n",
            "step 1100, loss: 4.6794\n",
            "step 1150, loss: 6.4132\n",
            "step 1200, loss: 6.1603\n",
            "step 1250, loss: 4.4089\n",
            "step 1300, loss: 4.6364\n",
            "step 1350, loss: 5.7759\n",
            "step 1400, loss: 5.3140\n",
            "step 1450, loss: 4.0079\n",
            "New best model saved! Loss: 4.0079\n",
            "step 1500, loss: 5.1795\n",
            "step 1550, loss: 4.7166\n",
            "step 1600, loss: 5.4516\n",
            "step 1650, loss: 5.3411\n",
            "step 1700, loss: 5.7764\n",
            "step 1750, loss: 4.2392\n",
            "step 1800, loss: 7.0087\n",
            "step 1850, loss: 6.6230\n",
            "step 1900, loss: 4.2996\n",
            "step 1950, loss: 6.1891\n",
            "step 2000, loss: 5.9996\n",
            "step 2050, loss: 4.6741\n",
            "step 2100, loss: 5.5739\n",
            "step 2150, loss: 4.7777\n",
            "step 2200, loss: 4.9316\n",
            "step 2250, loss: 4.1410\n",
            "step 2300, loss: 5.7114\n",
            "step 2350, loss: 4.8632\n",
            "step 2400, loss: 4.8294\n",
            "step 2450, loss: 4.5070\n",
            "step 2500, loss: 4.5423\n",
            "step 2550, loss: 5.8287\n",
            "step 2600, loss: 5.0692\n",
            "step 2650, loss: 5.4483\n",
            "step 2700, loss: 4.4396\n",
            "step 2750, loss: 5.4915\n",
            "step 2800, loss: 3.2503\n",
            "New best model saved! Loss: 3.2503\n",
            "step 2850, loss: 5.1243\n",
            "step 2900, loss: 5.4024\n",
            "step 2950, loss: 3.6271\n",
            "step 3000, loss: 3.5212\n",
            "step 3050, loss: 3.9840\n",
            "step 3100, loss: 4.6018\n",
            "step 3150, loss: 4.9010\n",
            "step 3200, loss: 4.2082\n",
            "step 3250, loss: 4.4325\n",
            "step 3300, loss: 4.5267\n",
            "step 3350, loss: 5.2010\n",
            "step 3400, loss: 5.2855\n",
            "step 3450, loss: 5.3494\n",
            "step 3500, loss: 5.5557\n",
            "step 3550, loss: 5.4043\n",
            "step 3600, loss: 5.0420\n",
            "step 3650, loss: 4.6230\n",
            "step 3700, loss: 4.8953\n",
            "step 3750, loss: 5.4358\n",
            "step 3800, loss: 4.2883\n",
            "step 3850, loss: 4.7328\n",
            "step 3900, loss: 4.7619\n",
            "step 3950, loss: 5.6038\n",
            "step 4000, loss: 4.6877\n",
            "step 4050, loss: 4.6643\n",
            "step 4100, loss: 4.8736\n",
            "step 4150, loss: 3.6975\n",
            "step 4200, loss: 4.6614\n",
            "step 4250, loss: 4.4969\n",
            "step 4300, loss: 3.5720\n",
            "step 4350, loss: 4.6945\n",
            "step 4400, loss: 4.1904\n",
            "step 4450, loss: 5.6078\n",
            "step 4500, loss: 4.2052\n",
            "step 4550, loss: 4.3695\n",
            "step 4600, loss: 3.7496\n",
            "step 4650, loss: 4.8681\n",
            "step 4700, loss: 3.4114\n",
            "step 4750, loss: 5.3952\n",
            "step 4800, loss: 3.1078\n",
            "New best model saved! Loss: 3.1078\n",
            "step 4850, loss: 4.1830\n",
            "step 4900, loss: 4.0963\n",
            "step 4950, loss: 5.0523\n",
            "step 5000, loss: 4.4717\n",
            "step 5050, loss: 3.9187\n",
            "step 5100, loss: 5.3284\n",
            "step 5150, loss: 4.8640\n",
            "step 5200, loss: 4.9390\n",
            "step 5250, loss: 4.5031\n",
            "step 5300, loss: 5.0085\n",
            "step 5350, loss: 4.5939\n",
            "step 5400, loss: 4.2560\n",
            "step 5450, loss: 5.0474\n",
            "step 5500, loss: 4.4703\n",
            "step 5550, loss: 3.3004\n",
            "step 5600, loss: 3.3679\n",
            "step 5650, loss: 5.1828\n",
            "step 5700, loss: 4.8325\n",
            "step 5750, loss: 3.1803\n",
            "step 5800, loss: 4.2458\n",
            "step 5850, loss: 3.0999\n",
            "New best model saved! Loss: 3.0999\n",
            "step 5900, loss: 4.4088\n",
            "step 5950, loss: 3.5951\n",
            "step 6000, loss: 3.6381\n",
            "step 6050, loss: 4.7232\n",
            "step 6100, loss: 4.7001\n",
            "step 6150, loss: 4.4736\n",
            "step 6200, loss: 4.4652\n",
            "step 6250, loss: 3.0707\n",
            "New best model saved! Loss: 3.0707\n",
            "step 6300, loss: 5.1478\n",
            "step 6350, loss: 4.1000\n",
            "step 6400, loss: 4.5652\n",
            "step 6450, loss: 5.5191\n",
            "step 6500, loss: 3.7340\n",
            "step 6550, loss: 4.0097\n",
            "step 6600, loss: 4.5299\n",
            "step 6650, loss: 4.6348\n",
            "step 6700, loss: 4.5804\n",
            "step 6750, loss: 5.3855\n",
            "step 6800, loss: 5.1772\n",
            "step 6850, loss: 3.5617\n",
            "step 6900, loss: 3.4022\n",
            "step 6950, loss: 4.4554\n",
            "step 7000, loss: 3.2525\n",
            "step 7050, loss: 4.9915\n",
            "step 7100, loss: 5.2654\n",
            "step 7150, loss: 3.7899\n",
            "step 7200, loss: 5.1196\n",
            "step 7250, loss: 4.4970\n",
            "step 7300, loss: 4.2730\n",
            "step 7350, loss: 5.1096\n",
            "step 7400, loss: 5.0957\n",
            "step 7450, loss: 4.4760\n",
            "step 7500, loss: 3.0383\n",
            "New best model saved! Loss: 3.0383\n",
            "step 7550, loss: 5.3181\n",
            "step 7600, loss: 4.1751\n",
            "step 7650, loss: 3.2584\n",
            "step 7700, loss: 3.2701\n",
            "step 7750, loss: 3.3935\n",
            "step 7800, loss: 3.3360\n",
            "step 7850, loss: 4.9050\n",
            "step 7900, loss: 3.8502\n",
            "step 7950, loss: 4.7087\n",
            "step 8000, loss: 4.6658\n",
            "step 8050, loss: 4.6130\n",
            "step 8100, loss: 3.4036\n",
            "step 8150, loss: 4.3372\n",
            "step 8200, loss: 3.4169\n",
            "step 8250, loss: 4.1092\n",
            "step 8300, loss: 4.0585\n",
            "step 8350, loss: 3.6534\n",
            "step 8400, loss: 3.7549\n",
            "step 8450, loss: 3.4693\n",
            "step 8500, loss: 3.1557\n",
            "step 8550, loss: 4.3958\n",
            "step 8600, loss: 4.7751\n",
            "step 8650, loss: 3.9184\n",
            "step 8700, loss: 3.7421\n",
            "step 8750, loss: 4.6956\n",
            "step 8800, loss: 4.3187\n",
            "step 8850, loss: 4.9168\n",
            "step 8900, loss: 4.3011\n",
            "step 8950, loss: 3.7586\n",
            "step 9000, loss: 4.1536\n",
            "step 9050, loss: 3.9679\n",
            "step 9100, loss: 3.8240\n",
            "step 9150, loss: 4.2583\n",
            "step 9200, loss: 4.5611\n",
            "step 9250, loss: 4.1539\n",
            "step 9300, loss: 2.7118\n",
            "New best model saved! Loss: 2.7118\n",
            "step 9350, loss: 4.7991\n",
            "step 9400, loss: 4.8467\n",
            "step 9450, loss: 4.2411\n",
            "step 9500, loss: 3.4651\n",
            "step 9550, loss: 5.0970\n",
            "step 9600, loss: 4.5330\n",
            "step 9650, loss: 3.9617\n",
            "step 9700, loss: 4.6783\n",
            "step 9750, loss: 4.7052\n",
            "step 9800, loss: 3.9937\n",
            "step 9850, loss: 4.6865\n",
            "step 9900, loss: 5.0015\n",
            "step 9950, loss: 3.5770\n",
            "step 10000, loss: 4.1152\n",
            "step 10050, loss: 4.9795\n",
            "step 10100, loss: 3.9153\n",
            "step 10150, loss: 2.2540\n",
            "New best model saved! Loss: 2.2540\n",
            "step 10200, loss: 4.9595\n",
            "step 10250, loss: 3.8651\n",
            "step 10300, loss: 3.1407\n",
            "step 10350, loss: 4.1581\n",
            "step 10400, loss: 3.2004\n",
            "step 10450, loss: 2.7809\n",
            "step 10500, loss: 4.2681\n",
            "step 10550, loss: 4.4277\n",
            "step 10600, loss: 4.9325\n",
            "step 10650, loss: 4.7979\n",
            "step 10700, loss: 4.3626\n",
            "step 10750, loss: 2.8130\n",
            "step 10800, loss: 2.8046\n",
            "step 10850, loss: 4.2824\n",
            "step 10900, loss: 3.9490\n",
            "step 10950, loss: 5.1731\n",
            "step 11000, loss: 3.1961\n",
            "step 11050, loss: 4.3567\n",
            "step 11100, loss: 2.9165\n",
            "step 11150, loss: 4.2122\n",
            "step 11200, loss: 4.9759\n",
            "step 11250, loss: 3.8991\n",
            "step 11300, loss: 4.4770\n",
            "step 11350, loss: 4.2100\n",
            "step 11400, loss: 3.9081\n",
            "step 11450, loss: 4.4641\n",
            "step 11500, loss: 4.1092\n",
            "step 11550, loss: 2.6116\n",
            "step 11600, loss: 3.8799\n",
            "step 11650, loss: 4.9885\n",
            "step 11700, loss: 3.5437\n",
            "step 11750, loss: 3.8357\n",
            "step 11800, loss: 4.2155\n",
            "step 11850, loss: 4.3207\n",
            "step 11900, loss: 4.0952\n",
            "step 11950, loss: 4.5594\n",
            "step 12000, loss: 3.6778\n",
            "step 12050, loss: 3.6970\n",
            "step 12100, loss: 3.7801\n",
            "step 12150, loss: 4.3112\n",
            "step 12200, loss: 3.0563\n",
            "step 12250, loss: 3.7885\n",
            "step 12300, loss: 3.8636\n",
            "step 12350, loss: 4.8087\n",
            "step 12400, loss: 4.9525\n",
            "step 12450, loss: 4.0959\n",
            "step 12500, loss: 4.3573\n",
            "step 12550, loss: 4.5902\n",
            "step 12600, loss: 4.3098\n",
            "step 12650, loss: 4.1454\n",
            "step 12700, loss: 3.6186\n",
            "step 12750, loss: 3.6909\n",
            "step 12800, loss: 3.4064\n",
            "step 12850, loss: 3.9541\n",
            "step 12900, loss: 3.2318\n",
            "step 12950, loss: 4.6026\n",
            "step 13000, loss: 4.0029\n",
            "step 13050, loss: 3.5079\n",
            "step 13100, loss: 3.4628\n",
            "step 13150, loss: 3.2202\n",
            "step 13200, loss: 3.8907\n",
            "step 13250, loss: 4.6873\n",
            "step 13300, loss: 4.1380\n",
            "step 13350, loss: 4.1537\n",
            "step 13400, loss: 3.3507\n",
            "step 13450, loss: 3.7338\n",
            "step 13500, loss: 4.5561\n",
            "step 13550, loss: 3.5664\n",
            "step 13600, loss: 2.9000\n",
            "step 13650, loss: 3.5018\n",
            "step 13700, loss: 4.0980\n",
            "step 13750, loss: 2.9368\n",
            "step 13800, loss: 4.0069\n",
            "step 13850, loss: 4.2698\n",
            "step 13900, loss: 3.7373\n",
            "step 13950, loss: 4.0597\n",
            "step 14000, loss: 3.3366\n",
            "step 14050, loss: 3.4396\n",
            "step 14100, loss: 4.4450\n",
            "step 14150, loss: 4.3787\n",
            "step 14200, loss: 3.6268\n",
            "step 14250, loss: 3.5986\n",
            "step 14300, loss: 3.3328\n",
            "step 14350, loss: 4.6958\n",
            "step 14400, loss: 4.8653\n",
            "step 14450, loss: 3.4664\n",
            "step 14500, loss: 3.7771\n",
            "step 14550, loss: 4.4360\n",
            "step 14600, loss: 3.6185\n",
            "step 14650, loss: 3.0551\n",
            "step 14700, loss: 4.4105\n",
            "step 14750, loss: 3.6767\n",
            "step 14800, loss: 4.4330\n",
            "step 14850, loss: 4.0860\n",
            "step 14900, loss: 4.5817\n",
            "step 14950, loss: 3.0191\n",
            "step 15000, loss: 5.4876\n",
            "step 15050, loss: 5.5409\n",
            "step 15100, loss: 3.5819\n",
            "step 15150, loss: 4.8774\n",
            "step 15200, loss: 4.3932\n",
            "step 15250, loss: 3.9066\n",
            "step 15300, loss: 4.5865\n",
            "step 15350, loss: 4.0192\n",
            "step 15400, loss: 3.9252\n",
            "step 15450, loss: 3.3243\n",
            "step 15500, loss: 4.0775\n",
            "step 15550, loss: 4.2661\n",
            "step 15600, loss: 3.9856\n",
            "step 15650, loss: 3.9191\n",
            "step 15700, loss: 3.9220\n",
            "step 15750, loss: 4.6885\n",
            "step 15800, loss: 3.9809\n",
            "step 15850, loss: 4.6256\n",
            "step 15900, loss: 3.5820\n",
            "step 15950, loss: 4.4049\n",
            "step 16000, loss: 2.6191\n",
            "step 16050, loss: 4.3918\n",
            "step 16100, loss: 4.6848\n",
            "step 16150, loss: 3.2451\n",
            "step 16200, loss: 3.0509\n",
            "step 16250, loss: 3.1940\n",
            "step 16300, loss: 3.8617\n",
            "step 16350, loss: 4.2572\n",
            "step 16400, loss: 3.5271\n",
            "step 16450, loss: 3.7143\n",
            "step 16500, loss: 3.8817\n",
            "step 16550, loss: 4.1703\n",
            "step 16600, loss: 4.6385\n",
            "step 16650, loss: 4.2533\n",
            "step 16700, loss: 4.6826\n",
            "step 16750, loss: 4.6220\n",
            "step 16800, loss: 4.3838\n",
            "step 16850, loss: 3.9778\n",
            "step 16900, loss: 4.2836\n",
            "step 16950, loss: 4.9163\n",
            "step 17000, loss: 3.5091\n",
            "step 17050, loss: 4.1609\n",
            "step 17100, loss: 4.2602\n",
            "step 17150, loss: 4.6046\n",
            "step 17200, loss: 3.8448\n",
            "step 17250, loss: 4.0983\n",
            "step 17300, loss: 4.1501\n",
            "step 17350, loss: 3.1873\n",
            "step 17400, loss: 3.9596\n",
            "step 17450, loss: 3.9824\n",
            "step 17500, loss: 2.9737\n",
            "step 17550, loss: 4.1188\n",
            "step 17600, loss: 3.7622\n",
            "step 17650, loss: 4.7058\n",
            "step 17700, loss: 3.5453\n",
            "step 17750, loss: 3.7194\n",
            "step 17800, loss: 3.3741\n",
            "step 17850, loss: 4.2235\n",
            "step 17900, loss: 3.0936\n",
            "step 17950, loss: 4.2679\n",
            "step 18000, loss: 2.8000\n",
            "step 18050, loss: 3.3483\n",
            "step 18100, loss: 3.7283\n",
            "step 18150, loss: 4.2269\n",
            "step 18200, loss: 3.9040\n",
            "step 18250, loss: 3.4455\n",
            "step 18300, loss: 4.5664\n",
            "step 18350, loss: 4.2877\n",
            "step 18400, loss: 3.9246\n",
            "step 18450, loss: 4.1068\n",
            "step 18500, loss: 4.2863\n",
            "step 18550, loss: 4.0972\n",
            "step 18600, loss: 3.9627\n",
            "step 18650, loss: 4.5153\n",
            "step 18700, loss: 4.0712\n",
            "step 18750, loss: 2.7879\n",
            "step 18800, loss: 3.1163\n",
            "step 18850, loss: 4.5662\n",
            "step 18900, loss: 4.2823\n",
            "step 18950, loss: 2.9125\n",
            "step 19000, loss: 3.5720\n",
            "step 19050, loss: 2.5546\n",
            "step 19100, loss: 4.0497\n",
            "step 19150, loss: 3.1192\n",
            "step 19200, loss: 3.1355\n",
            "step 19250, loss: 4.2776\n",
            "step 19300, loss: 4.0695\n",
            "step 19350, loss: 4.0547\n",
            "step 19400, loss: 3.9844\n",
            "step 19450, loss: 2.8132\n",
            "step 19500, loss: 4.6498\n",
            "step 19550, loss: 3.4157\n",
            "step 19600, loss: 4.2405\n",
            "step 19650, loss: 4.9027\n",
            "step 19700, loss: 3.3108\n",
            "step 19750, loss: 3.7101\n",
            "step 19800, loss: 4.0211\n",
            "step 19850, loss: 3.9274\n",
            "step 19900, loss: 4.0097\n",
            "step 19950, loss: 4.8318\n",
            "step 20000, loss: 4.6574\n",
            "step 20050, loss: 3.2478\n",
            "step 20100, loss: 3.0447\n",
            "step 20150, loss: 4.0274\n",
            "step 20200, loss: 3.0030\n",
            "step 20250, loss: 4.5735\n",
            "step 20300, loss: 4.5925\n",
            "step 20350, loss: 3.5783\n",
            "step 20400, loss: 4.8158\n",
            "step 20450, loss: 3.9128\n",
            "step 20500, loss: 3.8115\n",
            "step 20550, loss: 4.7976\n",
            "step 20600, loss: 4.8079\n",
            "step 20650, loss: 4.1861\n",
            "step 20700, loss: 2.8792\n",
            "step 20750, loss: 4.5526\n",
            "step 20800, loss: 3.7351\n",
            "step 20850, loss: 3.0076\n",
            "step 20900, loss: 2.8517\n",
            "step 20950, loss: 3.1263\n",
            "step 21000, loss: 3.1599\n",
            "step 21050, loss: 4.3337\n",
            "step 21100, loss: 3.2386\n",
            "step 21150, loss: 4.3553\n",
            "step 21200, loss: 4.5094\n",
            "step 21250, loss: 4.2458\n",
            "step 21300, loss: 3.1566\n",
            "step 21350, loss: 4.0170\n",
            "step 21400, loss: 3.1227\n",
            "step 21450, loss: 3.7285\n",
            "step 21500, loss: 3.6001\n",
            "step 21550, loss: 3.6180\n",
            "step 21600, loss: 3.6668\n",
            "step 21650, loss: 3.2022\n",
            "step 21700, loss: 2.7902\n",
            "step 21750, loss: 3.8974\n",
            "step 21800, loss: 4.4744\n",
            "step 21850, loss: 3.5005\n",
            "step 21900, loss: 3.3876\n",
            "step 21950, loss: 4.3275\n",
            "step 22000, loss: 3.9302\n",
            "step 22050, loss: 4.2594\n",
            "step 22100, loss: 3.9432\n",
            "step 22150, loss: 3.2641\n",
            "step 22200, loss: 3.8416\n",
            "step 22250, loss: 3.7700\n",
            "step 22300, loss: 3.5884\n",
            "step 22350, loss: 3.8909\n",
            "step 22400, loss: 4.2584\n",
            "step 22450, loss: 3.9588\n",
            "step 22500, loss: 2.5599\n",
            "step 22550, loss: 4.3065\n",
            "step 22600, loss: 4.0913\n",
            "step 22650, loss: 3.7099\n",
            "step 22700, loss: 2.7296\n",
            "step 22750, loss: 4.6742\n",
            "step 22800, loss: 4.0335\n",
            "step 22850, loss: 3.6481\n",
            "step 22900, loss: 4.2540\n",
            "step 22950, loss: 4.0435\n",
            "step 23000, loss: 3.7877\n",
            "step 23050, loss: 4.4400\n",
            "step 23100, loss: 4.4565\n",
            "step 23150, loss: 3.0398\n",
            "step 23200, loss: 3.9170\n",
            "step 23250, loss: 4.5605\n",
            "step 23300, loss: 3.5805\n",
            "step 23350, loss: 2.0016\n",
            "New best model saved! Loss: 2.0016\n",
            "step 23400, loss: 4.4644\n",
            "step 23450, loss: 3.4867\n",
            "step 23500, loss: 2.9207\n",
            "step 23550, loss: 3.8319\n",
            "step 23600, loss: 2.8513\n",
            "step 23650, loss: 2.5639\n",
            "step 23700, loss: 3.8132\n",
            "step 23750, loss: 4.0256\n",
            "step 23800, loss: 4.3275\n",
            "step 23850, loss: 4.3646\n",
            "step 23900, loss: 3.9813\n",
            "step 23950, loss: 2.7775\n",
            "step 24000, loss: 2.6529\n",
            "step 24050, loss: 3.8393\n",
            "step 24100, loss: 3.7882\n",
            "step 24150, loss: 4.5886\n",
            "step 24200, loss: 2.8733\n",
            "step 24250, loss: 4.1115\n",
            "step 24300, loss: 2.7342\n",
            "step 24350, loss: 3.9229\n",
            "step 24400, loss: 4.4495\n",
            "step 24450, loss: 3.6908\n",
            "step 24500, loss: 4.0263\n",
            "step 24550, loss: 3.8006\n",
            "step 24600, loss: 3.6148\n",
            "step 24650, loss: 4.1166\n",
            "step 24700, loss: 3.7425\n",
            "step 24750, loss: 2.5051\n",
            "step 24800, loss: 3.4269\n",
            "step 24850, loss: 4.3933\n",
            "step 24900, loss: 3.1973\n",
            "step 24950, loss: 3.4517\n",
            "step 25000, loss: 3.9748\n",
            "step 25050, loss: 4.1072\n",
            "step 25100, loss: 3.8617\n",
            "step 25150, loss: 4.0614\n",
            "step 25200, loss: 3.5755\n",
            "step 25250, loss: 3.4919\n",
            "step 25300, loss: 3.6032\n",
            "step 25350, loss: 3.9533\n",
            "step 25400, loss: 2.7677\n",
            "step 25450, loss: 3.5851\n",
            "step 25500, loss: 3.5632\n",
            "step 25550, loss: 4.3818\n",
            "step 25600, loss: 4.2875\n",
            "step 25650, loss: 3.8819\n",
            "step 25700, loss: 3.9584\n",
            "step 25750, loss: 4.1988\n",
            "step 25800, loss: 3.9090\n",
            "step 25850, loss: 3.8025\n",
            "step 25900, loss: 3.3223\n",
            "step 25950, loss: 3.4086\n",
            "step 26000, loss: 3.2264\n",
            "step 26050, loss: 3.8117\n",
            "step 26100, loss: 2.9604\n",
            "step 26150, loss: 4.4362\n",
            "step 26200, loss: 3.6890\n",
            "step 26250, loss: 3.2887\n",
            "step 26300, loss: 3.1827\n",
            "step 26350, loss: 2.8546\n",
            "step 26400, loss: 3.4928\n",
            "step 26450, loss: 4.3153\n",
            "step 26500, loss: 3.8548\n",
            "step 26550, loss: 4.0213\n",
            "step 26600, loss: 3.0780\n",
            "step 26650, loss: 3.4645\n",
            "step 26700, loss: 4.2186\n",
            "step 26750, loss: 3.3792\n",
            "step 26800, loss: 2.8407\n",
            "step 26850, loss: 3.3755\n",
            "step 26900, loss: 3.8240\n",
            "step 26950, loss: 2.8537\n",
            "step 27000, loss: 3.7031\n",
            "step 27050, loss: 3.8915\n",
            "step 27100, loss: 3.5980\n",
            "step 27150, loss: 3.7145\n",
            "step 27200, loss: 3.1981\n",
            "step 27250, loss: 3.1670\n",
            "step 27300, loss: 4.1227\n",
            "step 27350, loss: 4.0217\n",
            "step 27400, loss: 3.2747\n",
            "step 27450, loss: 3.4145\n",
            "step 27500, loss: 3.1939\n",
            "step 27550, loss: 4.3051\n",
            "step 27600, loss: 4.5423\n",
            "step 27650, loss: 3.3814\n",
            "step 27700, loss: 3.5123\n",
            "step 27750, loss: 4.1082\n",
            "step 27800, loss: 3.4318\n",
            "step 27850, loss: 3.0689\n",
            "step 27900, loss: 4.1711\n",
            "step 27950, loss: 3.3698\n",
            "step 28000, loss: 3.9641\n",
            "step 28050, loss: 3.7763\n",
            "step 28100, loss: 4.1595\n",
            "step 28150, loss: 2.7868\n",
            "step 28200, loss: 4.9853\n",
            "step 28250, loss: 4.9867\n",
            "step 28300, loss: 3.2788\n",
            "step 28350, loss: 4.3229\n",
            "step 28400, loss: 3.8985\n",
            "step 28450, loss: 3.8113\n",
            "step 28500, loss: 4.1802\n",
            "step 28550, loss: 3.8723\n",
            "step 28600, loss: 3.7804\n",
            "step 28650, loss: 3.4004\n",
            "step 28700, loss: 3.6968\n",
            "step 28750, loss: 4.0931\n",
            "step 28800, loss: 3.7598\n",
            "step 28850, loss: 3.7141\n",
            "step 28900, loss: 3.7606\n",
            "step 28950, loss: 4.4429\n",
            "step 29000, loss: 3.7349\n",
            "step 29050, loss: 4.0913\n",
            "step 29100, loss: 3.3796\n",
            "step 29150, loss: 3.9608\n",
            "step 29200, loss: 2.5379\n",
            "step 29250, loss: 4.1615\n",
            "step 29300, loss: 4.3564\n",
            "step 29350, loss: 3.1384\n",
            "step 29400, loss: 2.8655\n",
            "step 29450, loss: 3.0790\n",
            "step 29500, loss: 3.5215\n",
            "step 29550, loss: 4.0824\n",
            "step 29600, loss: 3.2030\n",
            "step 29650, loss: 3.5780\n",
            "step 29700, loss: 3.7759\n",
            "step 29750, loss: 4.0091\n",
            "step 29800, loss: 4.3204\n",
            "step 29850, loss: 4.1008\n",
            "step 29900, loss: 4.5259\n",
            "step 29950, loss: 4.2950\n",
            "step 30000, loss: 4.2705\n",
            "step 30050, loss: 3.6296\n",
            "step 30100, loss: 4.0050\n",
            "step 30150, loss: 4.7150\n",
            "step 30200, loss: 3.3847\n",
            "step 30250, loss: 3.9166\n",
            "step 30300, loss: 3.9438\n",
            "step 30350, loss: 4.1905\n",
            "step 30400, loss: 3.4705\n",
            "step 30450, loss: 3.8809\n",
            "step 30500, loss: 3.8299\n",
            "step 30550, loss: 3.1324\n",
            "step 30600, loss: 3.7854\n",
            "step 30650, loss: 3.7766\n",
            "step 30700, loss: 2.7889\n",
            "step 30750, loss: 3.8220\n",
            "step 30800, loss: 3.5086\n",
            "step 30850, loss: 4.4421\n",
            "step 30900, loss: 3.4596\n",
            "step 30950, loss: 3.5637\n",
            "step 31000, loss: 3.2863\n",
            "step 31050, loss: 4.0601\n",
            "step 31100, loss: 2.9841\n",
            "step 31150, loss: 3.9791\n",
            "step 31200, loss: 2.8821\n",
            "step 31250, loss: 3.2481\n",
            "step 31300, loss: 3.6928\n",
            "step 31350, loss: 4.0999\n",
            "step 31400, loss: 3.5736\n",
            "step 31450, loss: 3.3213\n",
            "step 31500, loss: 4.2686\n",
            "step 31550, loss: 4.1478\n",
            "step 31600, loss: 3.7665\n",
            "step 31650, loss: 3.8716\n",
            "step 31700, loss: 4.0303\n",
            "step 31750, loss: 3.9079\n",
            "step 31800, loss: 3.6612\n",
            "step 31850, loss: 4.2068\n",
            "step 31900, loss: 3.8549\n",
            "step 31950, loss: 2.5312\n",
            "step 32000, loss: 3.0314\n",
            "step 32050, loss: 4.1806\n",
            "step 32100, loss: 4.0822\n",
            "step 32150, loss: 2.8403\n",
            "step 32200, loss: 3.2883\n",
            "step 32250, loss: 2.4099\n",
            "step 32300, loss: 3.8053\n",
            "step 32350, loss: 2.9852\n",
            "step 32400, loss: 2.9784\n",
            "step 32450, loss: 4.0348\n",
            "step 32500, loss: 3.8866\n",
            "step 32550, loss: 3.8218\n",
            "step 32600, loss: 3.6238\n",
            "step 32650, loss: 2.7171\n",
            "step 32700, loss: 4.4339\n",
            "step 32750, loss: 3.1414\n",
            "step 32800, loss: 4.0318\n",
            "step 32850, loss: 4.7108\n",
            "step 32900, loss: 3.1801\n",
            "step 32950, loss: 3.5672\n",
            "step 33000, loss: 3.8829\n",
            "step 33050, loss: 3.7275\n",
            "step 33100, loss: 3.6575\n",
            "step 33150, loss: 4.4003\n",
            "step 33200, loss: 4.2796\n",
            "step 33250, loss: 3.1891\n",
            "step 33300, loss: 2.9775\n",
            "step 33350, loss: 3.9450\n",
            "step 33400, loss: 2.9062\n",
            "step 33450, loss: 4.1440\n",
            "step 33500, loss: 4.1983\n",
            "step 33550, loss: 3.4927\n",
            "step 33600, loss: 4.5501\n",
            "step 33650, loss: 3.8051\n",
            "step 33700, loss: 3.7215\n",
            "step 33750, loss: 4.5097\n",
            "step 33800, loss: 4.4718\n",
            "step 33850, loss: 4.0966\n",
            "step 33900, loss: 2.8011\n",
            "step 33950, loss: 4.2223\n",
            "step 34000, loss: 3.5689\n",
            "step 34050, loss: 3.0180\n",
            "step 34100, loss: 2.8368\n",
            "step 34150, loss: 2.9554\n",
            "step 34200, loss: 3.0240\n",
            "step 34250, loss: 3.9429\n",
            "step 34300, loss: 2.8508\n",
            "step 34350, loss: 4.1785\n",
            "step 34400, loss: 4.1702\n",
            "step 34450, loss: 4.1395\n",
            "step 34500, loss: 2.9843\n",
            "step 34550, loss: 3.9287\n",
            "step 34600, loss: 2.9772\n",
            "step 34650, loss: 3.6025\n",
            "step 34700, loss: 3.4404\n",
            "step 34750, loss: 3.4905\n",
            "step 34800, loss: 3.4016\n",
            "step 34850, loss: 2.9609\n",
            "step 34900, loss: 2.5329\n",
            "step 34950, loss: 3.5584\n",
            "step 35000, loss: 4.3139\n",
            "step 35050, loss: 3.3505\n",
            "step 35100, loss: 3.1643\n",
            "step 35150, loss: 4.1940\n",
            "step 35200, loss: 3.6149\n",
            "step 35250, loss: 3.7912\n",
            "step 35300, loss: 3.7190\n",
            "step 35350, loss: 3.0288\n",
            "step 35400, loss: 3.7458\n",
            "step 35450, loss: 3.6295\n",
            "step 35500, loss: 3.4484\n",
            "step 35550, loss: 3.6511\n",
            "step 35600, loss: 3.9882\n",
            "step 35650, loss: 3.7808\n",
            "step 35700, loss: 2.4456\n",
            "step 35750, loss: 4.1427\n",
            "step 35800, loss: 3.9581\n",
            "step 35850, loss: 3.6162\n",
            "step 35900, loss: 2.6165\n",
            "step 35950, loss: 4.2465\n",
            "step 36000, loss: 3.6509\n",
            "step 36050, loss: 3.5951\n",
            "step 36100, loss: 4.1470\n",
            "step 36150, loss: 3.9321\n",
            "step 36200, loss: 3.6444\n",
            "step 36250, loss: 4.1300\n",
            "step 36300, loss: 4.2621\n",
            "step 36350, loss: 2.9506\n",
            "step 36400, loss: 3.6193\n",
            "step 36450, loss: 4.0702\n",
            "step 36500, loss: 3.4908\n",
            "step 36550, loss: 1.9813\n",
            "New best model saved! Loss: 1.9813\n",
            "step 36600, loss: 4.1458\n",
            "step 36650, loss: 3.4807\n",
            "step 36700, loss: 2.7240\n",
            "step 36750, loss: 3.6088\n",
            "step 36800, loss: 2.4759\n",
            "step 36850, loss: 2.4345\n",
            "step 36900, loss: 3.6182\n",
            "step 36950, loss: 4.0329\n",
            "step 37000, loss: 4.0577\n",
            "step 37050, loss: 4.0411\n",
            "step 37100, loss: 3.7798\n",
            "step 37150, loss: 2.7293\n",
            "step 37200, loss: 2.5111\n",
            "step 37250, loss: 3.8511\n",
            "step 37300, loss: 3.7323\n",
            "step 37350, loss: 4.3379\n",
            "step 37400, loss: 2.8347\n",
            "step 37450, loss: 3.9003\n",
            "step 37500, loss: 2.6990\n",
            "step 37550, loss: 3.8530\n",
            "step 37600, loss: 4.0513\n",
            "step 37650, loss: 3.4002\n",
            "step 37700, loss: 3.7683\n",
            "step 37750, loss: 3.5731\n",
            "step 37800, loss: 3.2941\n",
            "step 37850, loss: 3.8319\n",
            "step 37900, loss: 3.6511\n",
            "step 37950, loss: 2.4690\n",
            "step 38000, loss: 3.0945\n",
            "step 38050, loss: 4.0255\n",
            "step 38100, loss: 3.1323\n",
            "step 38150, loss: 3.2028\n",
            "step 38200, loss: 3.8477\n",
            "step 38250, loss: 4.1081\n",
            "step 38300, loss: 3.6398\n",
            "step 38350, loss: 3.9218\n",
            "step 38400, loss: 3.3583\n",
            "step 38450, loss: 3.3165\n",
            "step 38500, loss: 3.3975\n",
            "step 38550, loss: 3.7284\n",
            "step 38600, loss: 2.6304\n",
            "step 38650, loss: 3.4631\n",
            "step 38700, loss: 3.3920\n",
            "step 38750, loss: 4.1459\n",
            "step 38800, loss: 3.9612\n",
            "step 38850, loss: 3.6421\n",
            "step 38900, loss: 3.6175\n",
            "step 38950, loss: 3.9304\n",
            "step 39000, loss: 3.6861\n",
            "step 39050, loss: 3.6232\n",
            "step 39100, loss: 3.3316\n",
            "step 39150, loss: 3.4444\n",
            "step 39200, loss: 3.2504\n",
            "step 39250, loss: 3.6881\n",
            "step 39300, loss: 2.7731\n",
            "step 39350, loss: 4.2768\n",
            "step 39400, loss: 3.3925\n",
            "step 39450, loss: 3.2568\n",
            "step 39500, loss: 2.9804\n",
            "step 39550, loss: 2.8260\n",
            "step 39600, loss: 3.3048\n",
            "step 39650, loss: 4.0846\n",
            "step 39700, loss: 3.5410\n",
            "step 39750, loss: 3.8250\n",
            "step 39800, loss: 3.0362\n",
            "step 39850, loss: 3.1829\n",
            "step 39900, loss: 4.0584\n",
            "step 39950, loss: 3.3391\n",
            "step 40000, loss: 2.6510\n",
            "step 40050, loss: 2.9619\n",
            "step 40100, loss: 3.5707\n",
            "step 40150, loss: 2.5631\n",
            "step 40200, loss: 3.3908\n",
            "step 40250, loss: 3.7800\n",
            "step 40300, loss: 3.4131\n",
            "step 40350, loss: 3.3954\n",
            "step 40400, loss: 3.1210\n",
            "step 40450, loss: 2.9411\n",
            "step 40500, loss: 3.8034\n",
            "step 40550, loss: 3.8349\n",
            "step 40600, loss: 3.0882\n",
            "step 40650, loss: 3.3334\n",
            "step 40700, loss: 2.9234\n",
            "step 40750, loss: 3.9011\n",
            "step 40800, loss: 3.8795\n",
            "step 40850, loss: 3.3380\n",
            "step 40900, loss: 3.2254\n",
            "step 40950, loss: 3.9547\n",
            "step 41000, loss: 3.0175\n",
            "step 41050, loss: 2.9054\n",
            "step 41100, loss: 4.1728\n",
            "step 41150, loss: 3.1931\n",
            "step 41200, loss: 3.8595\n",
            "step 41250, loss: 3.4252\n",
            "step 41300, loss: 4.1338\n",
            "step 41350, loss: 2.5422\n",
            "step 41400, loss: 4.6265\n",
            "step 41450, loss: 4.5946\n",
            "step 41500, loss: 3.1183\n",
            "step 41550, loss: 4.1430\n",
            "step 41600, loss: 3.6608\n",
            "step 41650, loss: 3.6241\n",
            "step 41700, loss: 4.0183\n",
            "step 41750, loss: 3.5999\n",
            "step 41800, loss: 3.4445\n",
            "step 41850, loss: 3.3127\n",
            "step 41900, loss: 3.3920\n",
            "step 41950, loss: 3.9053\n",
            "step 42000, loss: 3.4390\n",
            "step 42050, loss: 3.4251\n",
            "step 42100, loss: 3.6672\n",
            "step 42150, loss: 4.2296\n",
            "step 42200, loss: 3.4932\n",
            "step 42250, loss: 3.9257\n",
            "step 42300, loss: 3.3498\n",
            "step 42350, loss: 3.7219\n",
            "step 42400, loss: 2.4403\n",
            "step 42450, loss: 4.0560\n",
            "step 42500, loss: 4.1517\n",
            "step 42550, loss: 3.1846\n",
            "step 42600, loss: 2.9020\n",
            "step 42650, loss: 2.9276\n",
            "step 42700, loss: 3.3185\n",
            "step 42750, loss: 4.0035\n",
            "step 42800, loss: 3.2923\n",
            "step 42850, loss: 3.5649\n",
            "step 42900, loss: 3.4761\n",
            "step 42950, loss: 3.7600\n",
            "step 43000, loss: 4.0400\n",
            "step 43050, loss: 3.8556\n",
            "step 43100, loss: 4.2274\n",
            "step 43150, loss: 4.0462\n",
            "step 43200, loss: 3.9660\n",
            "step 43250, loss: 3.5212\n",
            "step 43300, loss: 3.8980\n",
            "step 43350, loss: 4.3198\n",
            "step 43400, loss: 3.0305\n",
            "step 43450, loss: 3.7059\n",
            "step 43500, loss: 3.8708\n",
            "step 43550, loss: 3.9136\n",
            "step 43600, loss: 3.4405\n",
            "step 43650, loss: 3.8071\n",
            "step 43700, loss: 3.6175\n",
            "step 43750, loss: 2.9009\n",
            "step 43800, loss: 3.6473\n",
            "step 43850, loss: 3.5569\n",
            "step 43900, loss: 2.6474\n",
            "step 43950, loss: 3.6211\n",
            "step 44000, loss: 3.4792\n",
            "step 44050, loss: 4.0013\n",
            "step 44100, loss: 3.0857\n",
            "step 44150, loss: 3.2394\n",
            "step 44200, loss: 3.0357\n",
            "step 44250, loss: 3.7078\n",
            "step 44300, loss: 2.9531\n",
            "step 44350, loss: 3.6131\n",
            "step 44400, loss: 2.7097\n",
            "step 44450, loss: 2.9453\n",
            "step 44500, loss: 3.3372\n",
            "step 44550, loss: 3.8274\n",
            "step 44600, loss: 3.4265\n",
            "step 44650, loss: 3.0634\n",
            "step 44700, loss: 3.9476\n",
            "step 44750, loss: 3.8992\n",
            "step 44800, loss: 3.3754\n",
            "step 44850, loss: 3.6229\n",
            "step 44900, loss: 3.8186\n",
            "step 44950, loss: 3.6834\n",
            "step 45000, loss: 3.5086\n",
            "step 45050, loss: 3.8663\n",
            "step 45100, loss: 3.4585\n",
            "step 45150, loss: 2.4050\n",
            "step 45200, loss: 2.9073\n",
            "step 45250, loss: 3.7728\n",
            "step 45300, loss: 3.7960\n",
            "step 45350, loss: 2.7987\n",
            "step 45400, loss: 3.1802\n",
            "step 45450, loss: 2.4270\n",
            "step 45500, loss: 3.6194\n",
            "step 45550, loss: 2.9099\n",
            "step 45600, loss: 3.0006\n",
            "step 45650, loss: 3.8970\n",
            "step 45700, loss: 3.8005\n",
            "step 45750, loss: 3.9184\n",
            "step 45800, loss: 3.5019\n",
            "step 45850, loss: 2.6758\n",
            "step 45900, loss: 4.2541\n",
            "step 45950, loss: 3.0542\n",
            "step 46000, loss: 3.9758\n",
            "step 46050, loss: 4.3531\n",
            "step 46100, loss: 2.8517\n",
            "step 46150, loss: 3.4532\n",
            "step 46200, loss: 3.6126\n",
            "step 46250, loss: 3.5743\n",
            "step 46300, loss: 3.7020\n",
            "step 46350, loss: 3.9774\n",
            "step 46400, loss: 4.1727\n",
            "step 46450, loss: 3.1396\n",
            "step 46500, loss: 2.9175\n",
            "step 46550, loss: 3.6672\n",
            "step 46600, loss: 2.7967\n",
            "step 46650, loss: 4.0059\n",
            "step 46700, loss: 3.8498\n",
            "step 46750, loss: 3.3001\n",
            "step 46800, loss: 4.2889\n",
            "step 46850, loss: 3.6916\n",
            "step 46900, loss: 3.5357\n",
            "step 46950, loss: 4.4826\n",
            "step 47000, loss: 4.4037\n",
            "step 47050, loss: 3.8666\n",
            "step 47100, loss: 2.8093\n",
            "step 47150, loss: 4.4013\n",
            "step 47200, loss: 3.7736\n",
            "step 47250, loss: 2.8874\n",
            "step 47300, loss: 2.7951\n",
            "step 47350, loss: 2.9692\n",
            "step 47400, loss: 3.0625\n",
            "step 47450, loss: 3.8880\n",
            "step 47500, loss: 2.7505\n",
            "step 47550, loss: 4.0515\n",
            "step 47600, loss: 4.1372\n",
            "step 47650, loss: 4.1148\n",
            "step 47700, loss: 2.8834\n",
            "step 47750, loss: 3.7497\n",
            "step 47800, loss: 3.1665\n",
            "step 47850, loss: 3.4717\n",
            "step 47900, loss: 3.3771\n",
            "step 47950, loss: 3.4278\n",
            "step 48000, loss: 3.3443\n",
            "step 48050, loss: 2.9659\n",
            "step 48100, loss: 2.5456\n",
            "step 48150, loss: 3.4436\n",
            "step 48200, loss: 4.3747\n",
            "step 48250, loss: 3.5076\n",
            "step 48300, loss: 3.1396\n",
            "step 48350, loss: 3.9867\n",
            "step 48400, loss: 3.4274\n",
            "step 48450, loss: 3.5146\n",
            "step 48500, loss: 3.7317\n",
            "step 48550, loss: 2.8181\n",
            "step 48600, loss: 3.5249\n",
            "step 48650, loss: 3.5124\n",
            "step 48700, loss: 3.2472\n",
            "step 48750, loss: 3.4475\n",
            "step 48800, loss: 3.8366\n",
            "step 48850, loss: 3.5560\n",
            "step 48900, loss: 2.5783\n",
            "step 48950, loss: 3.8894\n",
            "step 49000, loss: 3.5992\n",
            "step 49050, loss: 3.2097\n",
            "step 49100, loss: 2.4376\n",
            "step 49150, loss: 4.2155\n",
            "step 49200, loss: 3.2943\n",
            "step 49250, loss: 3.4757\n",
            "step 49300, loss: 3.7805\n",
            "step 49350, loss: 3.7582\n",
            "step 49400, loss: 3.4153\n",
            "step 49450, loss: 3.8481\n",
            "step 49500, loss: 4.1483\n",
            "step 49550, loss: 2.7753\n",
            "step 49600, loss: 3.6567\n",
            "step 49650, loss: 4.0028\n",
            "step 49700, loss: 3.4313\n",
            "step 49750, loss: 2.0905\n",
            "step 49800, loss: 4.1578\n",
            "step 49850, loss: 3.4075\n",
            "step 49900, loss: 2.6751\n",
            "step 49950, loss: 3.6026\n",
            "step 50000, loss: 2.4872\n",
            "step 50050, loss: 2.3533\n",
            "step 50100, loss: 3.5962\n",
            "step 50150, loss: 3.7612\n",
            "step 50200, loss: 3.7797\n",
            "step 50250, loss: 3.6757\n",
            "step 50300, loss: 3.5721\n",
            "step 50350, loss: 2.5749\n",
            "step 50400, loss: 2.5674\n",
            "step 50450, loss: 3.6378\n",
            "step 50500, loss: 3.6150\n",
            "step 50550, loss: 4.1115\n",
            "step 50600, loss: 2.8213\n",
            "step 50650, loss: 3.7829\n",
            "step 50700, loss: 2.7532\n",
            "step 50750, loss: 3.7381\n",
            "step 50800, loss: 3.9327\n",
            "step 50850, loss: 3.4206\n",
            "step 50900, loss: 3.5140\n",
            "step 50950, loss: 3.3344\n",
            "step 51000, loss: 3.3223\n",
            "step 51050, loss: 3.6511\n",
            "step 51100, loss: 3.3925\n",
            "step 51150, loss: 2.3112\n",
            "step 51200, loss: 3.0713\n",
            "step 51250, loss: 3.6132\n",
            "step 51300, loss: 3.0049\n",
            "step 51350, loss: 3.0868\n",
            "step 51400, loss: 3.6732\n",
            "step 51450, loss: 3.9970\n",
            "step 51500, loss: 3.7036\n",
            "step 51550, loss: 3.9320\n",
            "step 51600, loss: 3.1839\n",
            "step 51650, loss: 3.2290\n",
            "step 51700, loss: 3.3406\n",
            "step 51750, loss: 3.5857\n",
            "step 51800, loss: 2.5695\n",
            "step 51850, loss: 3.3912\n",
            "step 51900, loss: 3.2798\n",
            "step 51950, loss: 4.0929\n",
            "step 52000, loss: 3.6795\n",
            "step 52050, loss: 3.4809\n",
            "step 52100, loss: 3.7124\n",
            "step 52150, loss: 3.6472\n",
            "step 52200, loss: 3.6084\n",
            "step 52250, loss: 3.6063\n",
            "step 52300, loss: 3.2487\n",
            "step 52350, loss: 3.4470\n",
            "step 52400, loss: 3.3182\n",
            "step 52450, loss: 3.6447\n",
            "step 52500, loss: 2.6406\n",
            "step 52550, loss: 3.9543\n",
            "step 52600, loss: 3.2926\n",
            "step 52650, loss: 3.3187\n",
            "step 52700, loss: 2.9778\n",
            "step 52750, loss: 2.5829\n",
            "step 52800, loss: 3.2552\n",
            "step 52850, loss: 3.9172\n",
            "step 52900, loss: 3.5075\n",
            "step 52950, loss: 3.7388\n",
            "step 53000, loss: 3.0305\n",
            "step 53050, loss: 3.2082\n",
            "step 53100, loss: 3.9943\n",
            "step 53150, loss: 3.1466\n",
            "step 53200, loss: 2.6511\n",
            "step 53250, loss: 3.1237\n",
            "step 53300, loss: 3.5180\n",
            "step 53350, loss: 2.4593\n",
            "step 53400, loss: 3.1833\n",
            "step 53450, loss: 3.6197\n",
            "step 53500, loss: 3.3564\n",
            "step 53550, loss: 3.2894\n",
            "step 53600, loss: 3.1043\n",
            "step 53650, loss: 2.8963\n",
            "step 53700, loss: 3.7775\n",
            "step 53750, loss: 3.7683\n",
            "step 53800, loss: 2.9485\n",
            "step 53850, loss: 3.1812\n",
            "step 53900, loss: 3.0014\n",
            "step 53950, loss: 3.7337\n",
            "step 54000, loss: 3.8146\n",
            "step 54050, loss: 3.3651\n",
            "step 54100, loss: 3.3059\n",
            "step 54150, loss: 3.7201\n",
            "step 54200, loss: 3.0263\n",
            "step 54250, loss: 2.9852\n",
            "step 54300, loss: 4.0053\n",
            "step 54350, loss: 3.1049\n",
            "step 54400, loss: 3.5881\n",
            "step 54450, loss: 3.4190\n",
            "step 54500, loss: 3.7473\n",
            "step 54550, loss: 2.5377\n",
            "step 54600, loss: 4.4604\n",
            "step 54650, loss: 4.4705\n",
            "step 54700, loss: 3.1350\n",
            "step 54750, loss: 4.0065\n",
            "step 54800, loss: 3.5510\n",
            "step 54850, loss: 3.4319\n",
            "step 54900, loss: 4.1076\n",
            "step 54950, loss: 3.7272\n",
            "step 55000, loss: 3.4471\n",
            "step 55050, loss: 3.2539\n",
            "step 55100, loss: 3.6044\n",
            "step 55150, loss: 3.9502\n",
            "step 55200, loss: 3.5834\n",
            "step 55250, loss: 3.4368\n",
            "step 55300, loss: 3.6368\n",
            "step 55350, loss: 3.8599\n",
            "step 55400, loss: 3.3320\n",
            "step 55450, loss: 3.8400\n",
            "step 55500, loss: 3.3966\n",
            "step 55550, loss: 3.6813\n",
            "step 55600, loss: 2.4601\n",
            "step 55650, loss: 3.9059\n",
            "step 55700, loss: 4.2484\n",
            "step 55750, loss: 3.1863\n",
            "step 55800, loss: 2.7220\n",
            "step 55850, loss: 2.9486\n",
            "step 55900, loss: 3.3449\n",
            "step 55950, loss: 3.9863\n",
            "step 56000, loss: 3.1583\n",
            "step 56050, loss: 3.5109\n",
            "step 56100, loss: 3.4354\n",
            "step 56150, loss: 3.7044\n",
            "step 56200, loss: 4.0328\n",
            "step 56250, loss: 3.8742\n",
            "step 56300, loss: 3.9867\n",
            "step 56350, loss: 4.0702\n",
            "step 56400, loss: 4.0263\n",
            "step 56450, loss: 3.4482\n",
            "step 56500, loss: 3.7381\n",
            "step 56550, loss: 4.2838\n",
            "step 56600, loss: 2.8845\n",
            "step 56650, loss: 3.8244\n",
            "step 56700, loss: 3.6891\n",
            "step 56750, loss: 3.9185\n",
            "step 56800, loss: 3.2302\n",
            "step 56850, loss: 3.6766\n",
            "step 56900, loss: 3.6172\n",
            "step 56950, loss: 2.9799\n",
            "step 57000, loss: 3.5478\n",
            "step 57050, loss: 3.3173\n",
            "step 57100, loss: 2.6236\n",
            "step 57150, loss: 3.6473\n",
            "step 57200, loss: 3.4698\n",
            "step 57250, loss: 4.0088\n",
            "step 57300, loss: 3.2038\n",
            "step 57350, loss: 3.2568\n",
            "step 57400, loss: 2.9875\n",
            "step 57450, loss: 3.7423\n",
            "step 57500, loss: 2.7333\n",
            "step 57550, loss: 3.5048\n",
            "step 57600, loss: 2.6353\n",
            "step 57650, loss: 2.8713\n",
            "step 57700, loss: 3.3248\n",
            "step 57750, loss: 3.6952\n",
            "step 57800, loss: 3.3053\n",
            "step 57850, loss: 3.1658\n",
            "step 57900, loss: 3.9555\n",
            "step 57950, loss: 3.9238\n",
            "step 58000, loss: 3.4503\n",
            "step 58050, loss: 3.7216\n",
            "step 58100, loss: 3.7984\n",
            "step 58150, loss: 3.5785\n",
            "step 58200, loss: 3.4151\n",
            "step 58250, loss: 3.7873\n",
            "step 58300, loss: 3.4479\n",
            "step 58350, loss: 2.4817\n",
            "step 58400, loss: 2.9274\n",
            "step 58450, loss: 3.8107\n",
            "step 58500, loss: 3.7981\n",
            "step 58550, loss: 2.8048\n",
            "step 58600, loss: 3.1675\n",
            "step 58650, loss: 2.1548\n",
            "step 58700, loss: 3.4341\n",
            "step 58750, loss: 2.8452\n",
            "step 58800, loss: 2.7836\n",
            "step 58850, loss: 3.7120\n",
            "step 58900, loss: 3.5505\n",
            "step 58950, loss: 3.6488\n",
            "step 59000, loss: 3.3120\n",
            "step 59050, loss: 2.6131\n",
            "step 59100, loss: 4.0677\n",
            "step 59150, loss: 2.9664\n",
            "step 59200, loss: 3.9184\n",
            "step 59250, loss: 4.0423\n",
            "step 59300, loss: 2.9090\n",
            "step 59350, loss: 3.4713\n",
            "step 59400, loss: 3.6418\n",
            "step 59450, loss: 3.5034\n",
            "step 59500, loss: 3.5532\n",
            "step 59550, loss: 3.6586\n",
            "step 59600, loss: 3.9667\n",
            "step 59650, loss: 3.1424\n",
            "step 59700, loss: 2.7471\n",
            "step 59750, loss: 3.8440\n",
            "step 59800, loss: 2.7668\n",
            "step 59850, loss: 3.8647\n",
            "step 59900, loss: 3.8056\n",
            "step 59950, loss: 3.2591\n",
            "step 60000, loss: 4.1817\n",
            "step 60050, loss: 3.5112\n",
            "step 60100, loss: 3.3997\n",
            "step 60150, loss: 4.1066\n",
            "step 60200, loss: 3.9744\n",
            "step 60250, loss: 3.5517\n",
            "step 60300, loss: 2.6974\n",
            "step 60350, loss: 3.9610\n",
            "step 60400, loss: 3.4379\n",
            "step 60450, loss: 2.8163\n",
            "step 60500, loss: 2.7369\n",
            "step 60550, loss: 2.8212\n",
            "step 60600, loss: 2.7799\n",
            "step 60650, loss: 3.7134\n",
            "step 60700, loss: 2.5939\n",
            "step 60750, loss: 3.8972\n",
            "step 60800, loss: 4.0218\n",
            "step 60850, loss: 4.0735\n",
            "step 60900, loss: 2.8399\n",
            "step 60950, loss: 3.8976\n",
            "step 61000, loss: 2.8385\n",
            "step 61050, loss: 3.3205\n",
            "step 61100, loss: 3.1753\n",
            "step 61150, loss: 3.3866\n",
            "step 61200, loss: 3.2323\n",
            "step 61250, loss: 2.8196\n",
            "step 61300, loss: 2.4118\n",
            "step 61350, loss: 3.4931\n",
            "step 61400, loss: 4.1097\n",
            "step 61450, loss: 3.3845\n",
            "step 61500, loss: 2.8791\n",
            "step 61550, loss: 3.7329\n",
            "step 61600, loss: 3.2768\n",
            "step 61650, loss: 3.4247\n",
            "step 61700, loss: 3.6627\n",
            "step 61750, loss: 2.7342\n",
            "step 61800, loss: 3.5176\n",
            "step 61850, loss: 3.5175\n",
            "step 61900, loss: 3.1423\n",
            "step 61950, loss: 3.5239\n",
            "step 62000, loss: 3.6741\n",
            "step 62050, loss: 3.6661\n",
            "step 62100, loss: 2.4511\n",
            "step 62150, loss: 3.9156\n",
            "step 62200, loss: 3.6005\n",
            "step 62250, loss: 3.2588\n",
            "step 62300, loss: 2.4235\n",
            "step 62350, loss: 4.0315\n",
            "step 62400, loss: 3.2054\n",
            "step 62450, loss: 3.4364\n",
            "step 62500, loss: 3.5966\n",
            "step 62550, loss: 3.6049\n",
            "step 62600, loss: 3.3837\n",
            "step 62650, loss: 3.7469\n",
            "step 62700, loss: 4.0933\n",
            "step 62750, loss: 2.6025\n",
            "step 62800, loss: 3.5428\n",
            "step 62850, loss: 3.5126\n",
            "step 62900, loss: 3.0592\n",
            "step 62950, loss: 1.9523\n",
            "New best model saved! Loss: 1.9523\n",
            "step 63000, loss: 3.7511\n",
            "step 63050, loss: 3.3561\n",
            "step 63100, loss: 2.6786\n",
            "step 63150, loss: 3.5684\n",
            "step 63200, loss: 2.6663\n",
            "step 63250, loss: 2.3836\n",
            "step 63300, loss: 3.4100\n",
            "step 63350, loss: 3.5939\n",
            "step 63400, loss: 3.7227\n",
            "step 63450, loss: 3.6688\n",
            "step 63500, loss: 3.3532\n",
            "step 63550, loss: 2.5309\n",
            "step 63600, loss: 2.5116\n",
            "step 63650, loss: 3.5837\n",
            "step 63700, loss: 3.5054\n",
            "step 63750, loss: 3.9044\n",
            "step 63800, loss: 2.8040\n",
            "step 63850, loss: 3.5867\n",
            "step 63900, loss: 2.5787\n",
            "step 63950, loss: 3.5646\n",
            "step 64000, loss: 3.8074\n",
            "step 64050, loss: 3.1817\n",
            "step 64100, loss: 3.3042\n",
            "step 64150, loss: 3.1342\n",
            "step 64200, loss: 3.1548\n",
            "step 64250, loss: 3.6614\n",
            "step 64300, loss: 3.3174\n",
            "step 64350, loss: 2.3584\n",
            "step 64400, loss: 3.0498\n",
            "step 64450, loss: 3.4401\n",
            "step 64500, loss: 2.9621\n",
            "step 64550, loss: 3.1919\n",
            "step 64600, loss: 3.6227\n",
            "step 64650, loss: 3.8893\n",
            "step 64700, loss: 3.4138\n",
            "step 64750, loss: 3.5979\n",
            "step 64800, loss: 3.1625\n",
            "step 64850, loss: 3.0699\n",
            "step 64900, loss: 3.0785\n",
            "step 64950, loss: 3.4926\n",
            "step 65000, loss: 2.4376\n",
            "step 65050, loss: 3.3588\n",
            "step 65100, loss: 3.2514\n",
            "step 65150, loss: 4.1664\n",
            "step 65200, loss: 3.5534\n",
            "step 65250, loss: 3.5177\n",
            "step 65300, loss: 3.7867\n",
            "step 65350, loss: 3.5639\n",
            "step 65400, loss: 3.6260\n",
            "step 65450, loss: 3.5598\n",
            "step 65500, loss: 3.3326\n",
            "step 65550, loss: 3.1695\n",
            "step 65600, loss: 3.1850\n",
            "step 65650, loss: 3.5744\n",
            "step 65700, loss: 2.5870\n",
            "step 65750, loss: 3.8892\n",
            "step 65800, loss: 3.2198\n",
            "step 65850, loss: 2.9992\n",
            "step 65900, loss: 3.0187\n",
            "step 65950, loss: 2.5998\n",
            "step 66000, loss: 3.1187\n",
            "step 66050, loss: 3.6853\n",
            "step 66100, loss: 3.6726\n",
            "step 66150, loss: 3.6941\n",
            "step 66200, loss: 3.0527\n",
            "step 66250, loss: 3.0162\n",
            "step 66300, loss: 3.9109\n",
            "step 66350, loss: 3.1163\n",
            "step 66400, loss: 2.5266\n",
            "step 66450, loss: 2.8920\n",
            "step 66500, loss: 3.3272\n",
            "step 66550, loss: 2.4319\n",
            "step 66600, loss: 3.0680\n",
            "step 66650, loss: 3.4141\n",
            "step 66700, loss: 3.3489\n",
            "step 66750, loss: 3.2135\n",
            "step 66800, loss: 3.0394\n",
            "step 66850, loss: 2.8242\n",
            "step 66900, loss: 3.5504\n",
            "step 66950, loss: 3.6117\n",
            "step 67000, loss: 2.8897\n",
            "step 67050, loss: 3.1127\n",
            "step 67100, loss: 2.9150\n",
            "step 67150, loss: 3.5280\n",
            "step 67200, loss: 3.6835\n",
            "step 67250, loss: 3.3067\n",
            "step 67300, loss: 3.1693\n",
            "step 67350, loss: 3.6940\n",
            "step 67400, loss: 2.8733\n",
            "step 67450, loss: 3.0849\n",
            "step 67500, loss: 3.9336\n",
            "step 67550, loss: 3.1331\n",
            "step 67600, loss: 3.6196\n",
            "step 67650, loss: 3.2522\n",
            "step 67700, loss: 3.6909\n",
            "step 67750, loss: 2.5338\n",
            "step 67800, loss: 4.4188\n",
            "step 67850, loss: 4.4216\n",
            "step 67900, loss: 3.0669\n",
            "step 67950, loss: 4.0079\n",
            "step 68000, loss: 3.4238\n",
            "step 68050, loss: 3.3897\n",
            "step 68100, loss: 3.8537\n",
            "step 68150, loss: 3.5293\n",
            "step 68200, loss: 3.3163\n",
            "step 68250, loss: 3.1546\n",
            "step 68300, loss: 3.5204\n",
            "step 68350, loss: 3.9104\n",
            "step 68400, loss: 3.4044\n",
            "step 68450, loss: 3.3389\n",
            "step 68500, loss: 3.4689\n",
            "step 68550, loss: 3.9060\n",
            "step 68600, loss: 3.2935\n",
            "step 68650, loss: 3.7208\n",
            "step 68700, loss: 3.2943\n",
            "step 68750, loss: 3.4805\n",
            "step 68800, loss: 2.3786\n",
            "step 68850, loss: 3.8844\n",
            "step 68900, loss: 4.0910\n",
            "step 68950, loss: 2.8879\n",
            "step 69000, loss: 2.6568\n",
            "step 69050, loss: 2.6608\n",
            "step 69100, loss: 3.2342\n",
            "step 69150, loss: 3.7217\n",
            "step 69200, loss: 3.0861\n",
            "step 69250, loss: 3.4084\n",
            "step 69300, loss: 3.2121\n",
            "step 69350, loss: 3.4084\n",
            "step 69400, loss: 3.7798\n",
            "step 69450, loss: 3.6998\n",
            "step 69500, loss: 3.9007\n",
            "step 69550, loss: 3.9699\n",
            "step 69600, loss: 3.8069\n",
            "step 69650, loss: 3.3614\n",
            "step 69700, loss: 3.7972\n",
            "step 69750, loss: 4.2206\n",
            "step 69800, loss: 2.8737\n",
            "step 69850, loss: 3.5386\n",
            "step 69900, loss: 3.6699\n",
            "step 69950, loss: 3.7125\n",
            "step 70000, loss: 3.1401\n",
            "step 70050, loss: 3.5566\n",
            "step 70100, loss: 3.3206\n",
            "step 70150, loss: 2.8454\n",
            "step 70200, loss: 3.4707\n",
            "step 70250, loss: 3.4721\n",
            "step 70300, loss: 2.5404\n",
            "step 70350, loss: 3.5686\n",
            "step 70400, loss: 3.5429\n",
            "step 70450, loss: 4.0794\n",
            "step 70500, loss: 2.8664\n",
            "step 70550, loss: 3.1731\n",
            "step 70600, loss: 2.7786\n",
            "step 70650, loss: 3.6618\n",
            "step 70700, loss: 2.7287\n",
            "step 70750, loss: 3.4190\n",
            "step 70800, loss: 2.6624\n",
            "step 70850, loss: 2.8869\n",
            "step 70900, loss: 3.1587\n",
            "step 70950, loss: 3.6343\n",
            "step 71000, loss: 3.1544\n",
            "step 71050, loss: 2.9527\n",
            "step 71100, loss: 3.7308\n",
            "step 71150, loss: 3.8219\n",
            "step 71200, loss: 3.1985\n",
            "step 71250, loss: 3.5956\n",
            "step 71300, loss: 3.7056\n",
            "step 71350, loss: 3.5123\n",
            "step 71400, loss: 3.3242\n",
            "step 71450, loss: 3.7199\n",
            "step 71500, loss: 3.3852\n",
            "step 71550, loss: 2.5442\n",
            "step 71600, loss: 2.8249\n",
            "step 71650, loss: 3.5940\n",
            "step 71700, loss: 3.6087\n",
            "step 71750, loss: 2.7546\n",
            "step 71800, loss: 3.1795\n",
            "step 71850, loss: 2.0730\n",
            "step 71900, loss: 3.3931\n",
            "step 71950, loss: 2.7927\n",
            "step 72000, loss: 2.6887\n",
            "step 72050, loss: 3.8499\n",
            "step 72100, loss: 3.5105\n",
            "step 72150, loss: 3.6262\n",
            "step 72200, loss: 3.2566\n",
            "step 72250, loss: 2.6473\n",
            "step 72300, loss: 4.0133\n",
            "step 72350, loss: 2.9617\n",
            "step 72400, loss: 3.8326\n",
            "step 72450, loss: 4.1892\n",
            "step 72500, loss: 2.8535\n",
            "step 72550, loss: 3.2888\n",
            "step 72600, loss: 3.4869\n",
            "step 72650, loss: 3.4108\n",
            "step 72700, loss: 3.4934\n",
            "step 72750, loss: 3.4871\n",
            "step 72800, loss: 3.9406\n",
            "step 72850, loss: 3.0484\n",
            "step 72900, loss: 2.7057\n",
            "step 72950, loss: 3.6875\n",
            "step 73000, loss: 2.4636\n",
            "step 73050, loss: 3.8097\n",
            "step 73100, loss: 3.7761\n",
            "step 73150, loss: 3.2241\n",
            "step 73200, loss: 4.3222\n",
            "step 73250, loss: 3.5242\n",
            "step 73300, loss: 3.3529\n",
            "step 73350, loss: 4.1848\n",
            "step 73400, loss: 3.8226\n",
            "step 73450, loss: 3.3738\n",
            "step 73500, loss: 2.6742\n",
            "step 73550, loss: 3.7372\n",
            "step 73600, loss: 3.4003\n",
            "step 73650, loss: 2.8318\n",
            "step 73700, loss: 2.7609\n",
            "step 73750, loss: 2.8213\n",
            "step 73800, loss: 2.8399\n",
            "step 73850, loss: 3.4348\n",
            "step 73900, loss: 2.8386\n",
            "step 73950, loss: 3.8777\n",
            "step 74000, loss: 4.0799\n",
            "step 74050, loss: 4.1888\n",
            "step 74100, loss: 3.0268\n",
            "step 74150, loss: 3.6560\n",
            "step 74200, loss: 2.8568\n",
            "step 74250, loss: 3.6178\n",
            "step 74300, loss: 3.1971\n",
            "step 74350, loss: 3.3512\n",
            "step 74400, loss: 3.0230\n",
            "step 74450, loss: 2.7681\n",
            "step 74500, loss: 2.3866\n",
            "step 74550, loss: 3.3626\n",
            "step 74600, loss: 4.0297\n",
            "step 74650, loss: 3.1772\n",
            "step 74700, loss: 3.0704\n",
            "step 74750, loss: 3.9506\n",
            "step 74800, loss: 3.3049\n",
            "step 74850, loss: 3.3577\n",
            "step 74900, loss: 3.6120\n",
            "step 74950, loss: 2.5732\n",
            "step 75000, loss: 3.3858\n",
            "step 75050, loss: 3.4695\n",
            "step 75100, loss: 3.1472\n",
            "step 75150, loss: 3.3445\n",
            "step 75200, loss: 3.9483\n",
            "step 75250, loss: 3.6433\n",
            "step 75300, loss: 2.3629\n",
            "step 75350, loss: 3.6754\n",
            "step 75400, loss: 3.5510\n",
            "step 75450, loss: 3.0806\n",
            "step 75500, loss: 2.5510\n",
            "step 75550, loss: 3.8868\n",
            "step 75600, loss: 3.0371\n",
            "step 75650, loss: 3.4412\n",
            "step 75700, loss: 3.6708\n",
            "step 75750, loss: 3.7380\n",
            "step 75800, loss: 3.4842\n",
            "step 75850, loss: 3.7476\n",
            "step 75900, loss: 4.1170\n",
            "step 75950, loss: 2.8010\n",
            "step 76000, loss: 3.4918\n",
            "step 76050, loss: 3.6774\n",
            "step 76100, loss: 3.2366\n",
            "step 76150, loss: 1.9007\n",
            "New best model saved! Loss: 1.9007\n",
            "step 76200, loss: 3.8474\n",
            "step 76250, loss: 3.2116\n",
            "step 76300, loss: 2.5137\n",
            "step 76350, loss: 3.3771\n",
            "step 76400, loss: 2.4385\n",
            "step 76450, loss: 2.3460\n",
            "step 76500, loss: 3.3476\n",
            "step 76550, loss: 3.5228\n",
            "step 76600, loss: 3.7880\n",
            "step 76650, loss: 3.7367\n",
            "step 76700, loss: 3.5743\n",
            "step 76750, loss: 2.7069\n",
            "step 76800, loss: 2.4459\n",
            "step 76850, loss: 3.6746\n",
            "step 76900, loss: 3.8316\n",
            "step 76950, loss: 4.0767\n",
            "step 77000, loss: 2.7337\n",
            "step 77050, loss: 3.7606\n",
            "step 77100, loss: 2.6192\n",
            "step 77150, loss: 3.6456\n",
            "step 77200, loss: 4.0187\n",
            "step 77250, loss: 3.3379\n",
            "step 77300, loss: 3.5395\n",
            "step 77350, loss: 3.2476\n",
            "step 77400, loss: 3.3009\n",
            "step 77450, loss: 3.7145\n",
            "step 77500, loss: 3.5503\n",
            "step 77550, loss: 2.4599\n",
            "step 77600, loss: 3.0592\n",
            "step 77650, loss: 3.7030\n",
            "step 77700, loss: 3.1639\n",
            "step 77750, loss: 3.1179\n",
            "step 77800, loss: 4.2115\n",
            "step 77850, loss: 4.1189\n",
            "step 77900, loss: 3.5717\n",
            "step 77950, loss: 3.7359\n",
            "step 78000, loss: 3.2766\n",
            "step 78050, loss: 3.3734\n",
            "step 78100, loss: 3.3630\n",
            "step 78150, loss: 3.5978\n",
            "step 78200, loss: 2.6289\n",
            "step 78250, loss: 3.3337\n",
            "step 78300, loss: 3.2296\n",
            "step 78350, loss: 4.1112\n",
            "step 78400, loss: 3.7936\n",
            "step 78450, loss: 3.7126\n",
            "step 78500, loss: 3.6048\n",
            "step 78550, loss: 3.5097\n",
            "step 78600, loss: 3.6471\n",
            "step 78650, loss: 3.3482\n",
            "step 78700, loss: 3.1722\n",
            "step 78750, loss: 3.1022\n",
            "step 78800, loss: 3.1671\n",
            "step 78850, loss: 3.9184\n",
            "step 78900, loss: 2.6899\n",
            "step 78950, loss: 4.0736\n",
            "step 79000, loss: 3.3488\n",
            "step 79050, loss: 2.9765\n",
            "step 79100, loss: 3.0298\n",
            "step 79150, loss: 2.7036\n",
            "step 79200, loss: 3.0569\n",
            "step 79250, loss: 3.7657\n",
            "step 79300, loss: 3.5813\n",
            "step 79350, loss: 3.7841\n",
            "step 79400, loss: 3.1124\n",
            "step 79450, loss: 3.0093\n",
            "step 79500, loss: 3.9524\n",
            "step 79550, loss: 3.1509\n",
            "step 79600, loss: 2.5660\n",
            "step 79650, loss: 2.9302\n",
            "step 79700, loss: 3.3309\n",
            "step 79750, loss: 2.3861\n",
            "step 79800, loss: 3.2479\n",
            "step 79850, loss: 3.4437\n",
            "step 79900, loss: 3.2676\n",
            "step 79950, loss: 3.0725\n",
            "step 80000, loss: 3.1281\n",
            "step 80050, loss: 3.1179\n",
            "step 80100, loss: 3.8526\n",
            "step 80150, loss: 3.7004\n",
            "step 80200, loss: 2.8707\n",
            "step 80250, loss: 3.2623\n",
            "step 80300, loss: 2.9957\n",
            "step 80350, loss: 3.5311\n",
            "step 80400, loss: 3.6949\n",
            "step 80450, loss: 3.3021\n",
            "step 80500, loss: 2.9789\n",
            "step 80550, loss: 3.5244\n",
            "step 80600, loss: 2.8914\n",
            "step 80650, loss: 2.9427\n",
            "step 80700, loss: 3.8310\n",
            "step 80750, loss: 3.0833\n",
            "step 80800, loss: 3.5434\n",
            "step 80850, loss: 3.2585\n",
            "step 80900, loss: 3.7165\n",
            "step 80950, loss: 2.5775\n",
            "step 81000, loss: 4.4744\n",
            "step 81050, loss: 4.3690\n",
            "step 81100, loss: 3.0567\n",
            "step 81150, loss: 3.9035\n",
            "step 81200, loss: 3.5132\n",
            "step 81250, loss: 3.6605\n",
            "step 81300, loss: 4.0086\n",
            "step 81350, loss: 3.7794\n",
            "step 81400, loss: 3.3987\n",
            "step 81450, loss: 3.1954\n",
            "step 81500, loss: 3.5993\n",
            "step 81550, loss: 3.9247\n",
            "step 81600, loss: 3.3567\n",
            "step 81650, loss: 3.3995\n",
            "step 81700, loss: 3.5179\n",
            "step 81750, loss: 3.9323\n",
            "step 81800, loss: 3.4130\n",
            "step 81850, loss: 3.5461\n",
            "step 81900, loss: 3.3049\n",
            "step 81950, loss: 3.5738\n",
            "step 82000, loss: 2.5619\n",
            "step 82050, loss: 4.0832\n",
            "step 82100, loss: 3.9766\n",
            "step 82150, loss: 2.9958\n",
            "step 82200, loss: 2.7846\n",
            "step 82250, loss: 2.8212\n",
            "step 82300, loss: 3.2686\n",
            "step 82350, loss: 3.8797\n",
            "step 82400, loss: 3.1155\n",
            "step 82450, loss: 3.4278\n",
            "step 82500, loss: 3.2226\n",
            "step 82550, loss: 3.3885\n",
            "step 82600, loss: 3.8924\n",
            "step 82650, loss: 3.6213\n",
            "step 82700, loss: 4.1188\n",
            "step 82750, loss: 3.9892\n",
            "step 82800, loss: 3.8224\n",
            "step 82850, loss: 3.5059\n",
            "step 82900, loss: 3.8404\n",
            "step 82950, loss: 4.1580\n",
            "step 83000, loss: 2.9537\n",
            "step 83050, loss: 3.7898\n",
            "step 83100, loss: 3.5904\n",
            "step 83150, loss: 3.8896\n",
            "step 83200, loss: 3.3040\n",
            "step 83250, loss: 3.8063\n",
            "step 83300, loss: 3.3991\n",
            "step 83350, loss: 2.9197\n",
            "step 83400, loss: 3.3748\n",
            "step 83450, loss: 3.4625\n",
            "step 83500, loss: 2.6684\n",
            "step 83550, loss: 3.5802\n",
            "step 83600, loss: 3.5185\n",
            "step 83650, loss: 3.8413\n",
            "step 83700, loss: 3.1219\n",
            "step 83750, loss: 3.2785\n",
            "step 83800, loss: 2.8971\n",
            "step 83850, loss: 3.5126\n",
            "step 83900, loss: 2.8772\n",
            "step 83950, loss: 3.4289\n",
            "step 84000, loss: 2.7553\n",
            "step 84050, loss: 3.0532\n",
            "step 84100, loss: 3.2207\n",
            "step 84150, loss: 3.5526\n",
            "step 84200, loss: 3.2943\n",
            "step 84250, loss: 3.1795\n",
            "step 84300, loss: 3.7714\n",
            "step 84350, loss: 3.8136\n",
            "step 84400, loss: 3.2136\n",
            "step 84450, loss: 3.5066\n",
            "step 84500, loss: 3.5168\n",
            "step 84550, loss: 3.5543\n",
            "step 84600, loss: 3.2835\n",
            "step 84650, loss: 4.0125\n",
            "step 84700, loss: 3.4203\n",
            "step 84750, loss: 2.4240\n",
            "step 84800, loss: 2.9087\n",
            "step 84850, loss: 3.7275\n",
            "step 84900, loss: 3.5579\n",
            "step 84950, loss: 2.7139\n",
            "step 85000, loss: 3.2904\n",
            "step 85050, loss: 2.1818\n",
            "step 85100, loss: 3.4878\n",
            "step 85150, loss: 2.8193\n",
            "step 85200, loss: 2.7670\n",
            "step 85250, loss: 3.8890\n",
            "step 85300, loss: 3.7451\n",
            "step 85350, loss: 3.6255\n",
            "step 85400, loss: 3.4617\n",
            "step 85450, loss: 2.6917\n",
            "step 85500, loss: 4.0743\n",
            "step 85550, loss: 3.1261\n",
            "step 85600, loss: 3.8532\n",
            "step 85650, loss: 4.2164\n",
            "step 85700, loss: 2.8962\n",
            "step 85750, loss: 3.2862\n",
            "step 85800, loss: 3.7154\n",
            "step 85850, loss: 3.4455\n",
            "step 85900, loss: 3.4246\n",
            "step 85950, loss: 3.6613\n",
            "step 86000, loss: 4.1176\n",
            "step 86050, loss: 2.9876\n",
            "step 86100, loss: 2.7553\n",
            "step 86150, loss: 3.6920\n",
            "step 86200, loss: 2.5634\n",
            "step 86250, loss: 3.8371\n",
            "step 86300, loss: 3.7092\n",
            "step 86350, loss: 3.1420\n",
            "step 86400, loss: 4.0841\n",
            "step 86450, loss: 3.5384\n",
            "step 86500, loss: 3.4406\n",
            "step 86550, loss: 4.0006\n",
            "step 86600, loss: 3.8532\n",
            "step 86650, loss: 3.5151\n",
            "step 86700, loss: 2.6798\n",
            "step 86750, loss: 3.7523\n",
            "step 86800, loss: 3.4206\n",
            "step 86850, loss: 2.7486\n",
            "step 86900, loss: 2.8352\n",
            "step 86950, loss: 2.9630\n",
            "step 87000, loss: 2.6583\n",
            "step 87050, loss: 3.4894\n",
            "step 87100, loss: 2.5229\n",
            "step 87150, loss: 3.9854\n"
          ]
        }
      ]
    }
  ]
}